version: "3.8"

services:

  mongo:
    image: mongo:4.0
    container_name: mongo
    ports:
      - "27017:27017"
    volumes:
    - ./resources:/resources
    networks:
      - default
      
  mongo-init:
    image: mongo:4.0
    depends_on:
      - mongo
    volumes:
      - ./resources:/resources
    entrypoint: [ "bash", "-c", "sleep 10 && /resources/import_distances.sh" ]
    networks:
      - default
      

  kafka:
    image: bitnami/kafka:3.9.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
    networks:
     - default


  nifi:
    image: apache/nifi:1.23.2
    container_name: nifi
    ports: 
      - "8443:8443"
    environment: 
      - NIFI_WEB_HTTP_PORT=8443
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=josepablo
    depends_on:
      - kafka
    networks:
      - default
    
  flask:
    build: ./resources/web
    container_name: flask
    ports:
      - "5001:5001"
    depends_on:
      - kafka
      - mongo
    environment:
    - PROJECT_HOME=/app
    networks:
      - default
    
    
  spark-master:
    image: bitnami/spark:3.5.3
    container_name: spark-master
    depends_on:
      - kafka
      - mongo
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
    volumes:
      - ./models:/models
      - ./flight_prediction:/flight_prediction
    networks:
      - default  
      

  spark-worker-1:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_MODE=worker
      - PYSPARK_PYTHON=python3       
      - HOME=/tmp 
    volumes:
      - ./models:/models
      - ./flight_prediction:/flight_prediction
    networks:
      - default  
      
  spark-worker-2:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_MODE=worker
      - PYSPARK_PYTHON=python3      
      - HOME=/tmp 
    volumes:
      - ./models:/models
      - ./flight_prediction:/flight_prediction
    networks:
      - default  
      
  spark-submit:
    image: bitnami/spark:3.5.3
    container_name: spark-submit
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    ports:
      - "4040:4040"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "constraint:node==spark-master"
      
      - PYSPARK_PYTHON=python3       
      - HOME=/tmp 
    volumes:
      - ./flight_prediction:/flight_prediction
      - ./models:/models  
      - ./wait_for_model_and_run.sh:/wait_for_model_and_run.sh  
    command: >
      bash /wait_for_model_and_run.sh 
    networks:
      - default  
      

  namenode1:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode1
    ports:
      - "9870:9870"   
      - "8020:8020"
    environment:
      - CLUSTER_NAME=example1
      - INIT_DAEMON_STEP=setup_hdfs
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_disk_balancer_enabled=false
    volumes:
      - namenode1:/hadoop/dfs/name
    networks:
      - default

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode1:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    volumes:
      - datanode1:/hadoop/dfs/data
    depends_on:
      - namenode1
    networks:
      - default
      
  airflow:
    build:
      context: .
      dockerfile: Dockerfile_airflow 
    container_name: airflow_container
    environment:
      - AIRFLOW_HOME=/airflow
      - SPARK_HOME=/opt/spark
    ports:
      - "8180:8180"   
    
    networks:
      - default
    command: bash -c "airflow db init && \
      airflow users create --username admin --firstname Jose --lastname Pablo --role Admin --email example@email.org --password admin && \
      supervisord"
    volumes:
      - ./models:/models
        
      
      
networks:
  default:
    driver: bridge
    
volumes:
  namenode1:
  datanode1:
