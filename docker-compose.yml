version: "3.8"

services:

  mongo:
    image: mongo:4.0
    container_name: mongo
    ports:
      - "27017:27017"
    volumes:
    - ./resources:/resources
  mongo-init:
    image: mongo:4.0
    depends_on:
      - mongo
    volumes:
      - ./resources:/resources
    entrypoint: [ "bash", "-c", "sleep 10 && /resources/import_distances.sh" ]
      

  kafka:
    image: bitnami/kafka:3.9.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_KRAFT_CLUSTER_ID=abcdefghijklmnopqrstuv
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
    networks:
     - default


  nifi:
    image: apache/nifi:1.23.2
    container_name: nifi
    ports: 
      - "8443:8443"
    environment: 
      - NIFI_WEB_HTTP_PORT=8443
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=josepablo
    depends_on:
      - kafka
    networks:
      - default
    volumes:
      - ./nifi_bueno:/opt/nifi/nifi-current/output-data
    
    
    
  flask:
    build: ./resources/web
    container_name: flask
    ports:
      - "5001:5001"
    depends_on:
      - kafka
      - mongo
    environment:
    - PROJECT_HOME=/app
    networks:
      - default
    
    
  spark-master:
    image: bitnami/spark:3.5.3
    container_name: spark-master
    depends_on:
      - kafka
      - mongo
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
    volumes:
      - ./models:/models
      - ./flight_prediction:/flight_prediction
      

  spark-worker-1:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_MODE=worker
      - PYSPARK_PYTHON=python3       
      - HOME=/tmp 
    volumes:
      - ./models:/models
      - ./flight_prediction:/flight_prediction
      
  spark-worker-2:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - SPARK_MODE=worker
      - PYSPARK_PYTHON=python3      
      - HOME=/tmp 
    volumes:
      - ./models:/models
      - ./flight_prediction:/flight_prediction
      
  spark-submit:
    image: bitnami/spark:3.5.3
    container_name: spark-submit
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
    ports:
      - "4040:4040"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "constraint:node==spark-master"
      - "SERVER=${SERVER}"
      - PYSPARK_PYTHON=python3       
      - HOME=/tmp 
    command: >
      spark-submit --master spark://spark-master:7077 --conf spark.jars.ivy=/tmp/.ivy2 --packages org.mongodb.spark:mongo-spark-connector_2.12:10.4.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 --class es.upm.dit.ging.predictor.MakePrediction /flight_prediction/target/scala-2.12/flight_prediction_2.12-0.1.jar

    volumes:
      - ./flight_prediction:/flight_prediction
      - ./models:/models
      

  namenode1:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode1
    ports:
      - "50070:50070"   # Interfaz web del Namenode
      - "9870:9870"     # Interfaz web del Namenode HDFS
    environment:
      - CLUSTER_NAME=example1
      - INIT_DAEMON_STEP=setup_hdfs
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
    volumes:
      - ./volumes/namenode1:/hadoop/dfs/name
      - ./init-namenode.sh:/init-namenode.sh
    networks:
      - default
    entrypoint: ["/bin/bash", "/init-namenode.sh"]

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode1:8020
      - HDFS_CONF_dfs_webhdfs_enabled=true
    ports:
      - "50075:50075"   # Interfaz web del Datanode
      - "9864:9864"     # Puerto HDFS del Datanode
    volumes:
      - ./volumes/datanode1:/hadoop/dfs/data
    networks:
      - default
 
networks:
  default:
    driver: bridge
